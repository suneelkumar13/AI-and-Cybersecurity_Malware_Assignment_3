# -*- coding: utf-8 -*-
"""Selecting the Best N-grams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YaFSSBo6kAMfyuMEayLXhm35G35hrxlg

In this exercise, we perform feature extraction, selection and vectorization for binary files using the frequency, mutual information, and Chi test methods available in the SKLearn library.
"""

#To install sklearn, uncomment and run the following line:
#!pip install sklearn

import collections
from nltk import ngrams
import numpy as np

def readFile(filePath):
    with open(filePath, "rb") as binary_file:
        data = binary_file.read()
    return data

def byteSequenceToNgrams(byteSequence, n):
    Ngrams = ngrams(byteSequence, n)
    return list(Ngrams)
    
def extractNgramCounts(file, N):
    fileByteSequence = readFile(file)
    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)
    return collections.Counter(fileNgrams)

from os import listdir
from os.path import isfile, join
directories = ["Samples/Benign", "Samples/Malware"]
#Assign the value of the N in N-grams
N=2

#This may take a few minutes to run
totalNgramCount = collections.Counter([])
for datasetPath in directories:
    samples = [f for f in listdir(datasetPath) if isfile(join(datasetPath,f))]
    for file in samples:
        filePath = join(datasetPath, file)
        totalNgramCount += extractNgramCounts(filePath, N)

#Extract the list of top 1000 most common N-Grams
K1 = 1000
K1_most_common_Ngrams = totalNgramCount.most_common(K1)
K1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]

K1_most_common_Ngrams_list

# Create a vector fv[] of the most frequent n-grams
def featurizeSample(file, K1_most_common_Ngrams_list):
    K1 = len(K1_most_common_Ngrams_list)
    fv = K1*[0]
    fileNgrams = extractNgramCounts(file, N)
    for i in range(K1):
        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]
    return fv

# Create a labeled dataset of frequency-based feature vectors 
# for benign and malware samples.
# This may take a few minutes to run.
directoriesWithLabels = [("Samples/Benign",0), ("Samples/Malware",1)]
X = []
y = []
fileNum = 0
for datasetPath, label in directoriesWithLabels:
    samples = [f for f in listdir(datasetPath) if isfile(join(datasetPath,f))]
    for file in samples:
        fileNum +=1
        filePath = join(datasetPath, file)
        X.append(featurizeSample(filePath, K1_most_common_Ngrams_list))
        y.append(label)

X = np.asarray(X)

from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2
K2 = 100

#Select the 100 most frequent features in each sample
X_top_K2_freq = X[:,:K2]

#Feature selection based on the mutual information method
mi_selector = SelectKBest(mutual_info_classif, k=K2)
X_top_K2_mi = mi_selector.fit_transform(X, y)

#Feature selection based on the Chi test method
chi2_selector = SelectKBest(chi2, k=K2)
X_top_K2_ch2 = chi2_selector.fit_transform(X, y)

"""**Exercise:** How many feature vectors in X_top_K2_ch2 are the same as their corresponding feature vectors in X_top_K2_freq? (i.e., X_top_K2_ch2[i] == X_top_K2_freq[i]). Calculate the same between X_top_K2_mi and X_top_K2_ch2, as well as between X_top_K2_mi and X_top_K2_freq."""

# Your code.
def same_feat_count(list_1, list_2):
    count = len(list_1[0,:]) + len(list_2[0,:]) - len(set(list_1[0,:]+list_2[0,:])) # Total element - unique elements
    return count
    

same_feat_count(X_top_K2_freq, X_top_K2_ch2)

same_feat_count(X_top_K2_mi, X_top_K2_freq)

